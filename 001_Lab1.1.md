# Lab: Building a Simple Chatbot with LangGraph and LangChain OpenAI

---

## Learning Objectives

By the end of this lab, participants will be able to:

1. **Install and configure** the required Python packages (`langgraph`, `langchain-openai`) in a notebook environment.
2. **Set and use** the OpenAI API key securely via environment variables.
3. **Define a state structure** (`ConversationState`) to hold data passed between nodes in a LangGraph workflow.
4. **Create node functions** that:

   * Prepare user input for the chatbot, and
   * Call an OpenAI model to generate a response.
5. **Build and wire up a LangGraph graph**, including nodes, edges, and an entry point.
6. **Compile and run** the graph using `graph.invoke()` and inspect the returned state (chatbot response and messages list).
7. **Explain the end-to-end data flow**, from initial state → nodes → final output, in their own words.

---

## Step-by-Step Lab Guide (with Explanation)

> **Important:** Learners should copy the code *exactly as given* into a Jupyter / Colab notebook cell, and only change the API key value to their own key.

---

### Step 0 – Lab Setup

**What to do**

1. Open **Google Colab** or a local **Jupyter Notebook**.
2. Create a new notebook named: `LangGraph_Simple_Chatbot.ipynb`.

**Why this matters**

* LangGraph workflows are easiest to demonstrate in a notebook environment where you can run cells step by step and inspect outputs.
* Having a clean notebook keeps the lab focused and easy to debug.

---

### Step 1 – Install and Import Dependencies

```python
# Step 1: Install and import dependencies
!pip install langgraph langchain-openai

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "xxx"

import os
from langgraph.graph import StateGraph, END
from typing import Dict, Any
from langchain_openai import ChatOpenAI
```

**What to do**

1. Paste this block into the **first cell** of your notebook.
2. Replace `"xxx"` with **your own OpenAI API key** (never share this key publicly).
3. Run the cell.

**What you should see**

* `pip` will download and install `langgraph` and `langchain-openai`.
* If everything is correct, the cell finishes without errors.

**Explanation**

* `!pip install ...` installs the required libraries **inside your notebook environment**.
* `os.environ["OPENAI_API_KEY"] = "xxx"` sets an **environment variable** that `langchain-openai` will automatically read when creating `ChatOpenAI`.
* `StateGraph` and `END` come from `langgraph.graph` and are used to build and finalize the graph.
* `ChatOpenAI` is a LangChain wrapper around OpenAI’s chat models, making it easy to call models like `gpt-3.5-turbo`.

> **Side note (good practice):** In real projects, it’s better to store API keys in `.env` files or secret managers, not hard-coded in notebooks you share.

---

### Step 2 – Define the State Structure

```python
# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "xxx"

# Step 2: Define state structure
class ConversationState(Dict[str, Any]):
    messages: list
    user_input: str
    response: str
```

**What to do**

1. Paste this block into the **next cell**.
2. (Strongly recommended) Replace the long API key value with **your own key**, and keep it private.
3. Run the cell.

**What you should see**

* No printed output, but the `ConversationState` type will now be available.
* The environment variable for the API key is updated again (this just overwrites the previous value).

**Explanation**

* `ConversationState` inherits from `Dict[str, Any]`, meaning the graph state is treated as a **dictionary**.
* The annotations:

  * `messages: list` – will store conversation history as a list (e.g., `[user_message, bot_response]`).
  * `user_input: str` – holds the current question or prompt from the user.
  * `response: str` – holds the model’s latest response.

This state object is the **shared memory** passed through the nodes in LangGraph. Each node reads and/or updates parts of this dictionary.

---

### Step 3 – Create Node Functions

```python
# Step 3: Create node functions
def user_input_node(state: ConversationState):
    return {"user_input": "Tell me where is Malaysia?"}

def chatbot_node(state: ConversationState):
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    response = llm.invoke(state["user_input"])
    return {"response": response.content, "messages": [state["user_input"], response.content]}
```

**What to do**

1. Paste this code into a **new cell**.
2. Run the cell.

**What you should see**

* Again, no direct printed output, but two functions are now defined:

  * `user_input_node`
  * `chatbot_node`

**Explanation**

* `user_input_node(state: ConversationState)`

  * Ignores the current state and **returns a new piece of state**:

    ```python
    {"user_input": "Tell me where is Malaysia?"}
    ```
  * In this simple lab, the user input is **hard-coded**, which keeps the example deterministic and easy to understand.
  * This function acts as the "user" in the conversation.

* `chatbot_node(state: ConversationState)`

  * Creates a `ChatOpenAI` object for model `"gpt-3.5-turbo"`.
  * Calls `llm.invoke(state["user_input"])` to send the question to the model.
  * Extracts `response.content`, which is the model’s text reply.
  * Returns:

    ```python
    {
      "response": response.content,
      "messages": [state["user_input"], response.content]
    }
    ```
  * This updates the state with:

    * `response`: the latest reply from the model.
    * `messages`: a simple list storing the user question and the model answer.

This pair of node functions forms a **mini conversation pipeline**: one node prepares the question, the other talks to the LLM.

---

### Step 4 – Build the Graph

```python
# Step 4: Build the graph
builder = StateGraph(ConversationState)
builder.add_node("get_input", user_input_node)
builder.add_node("generate_response", chatbot_node)
```

**What to do**

1. Paste into a **new cell** and run it.

**What you should see**

* No output, but:

  * `builder` is now a `StateGraph` builder that knows the shape of the state (`ConversationState`).
  * Two nodes have been registered:

    * `"get_input"` → `user_input_node`
    * `"generate_response"` → `chatbot_node`

**Explanation**

* `StateGraph(ConversationState)` tells LangGraph: *“This graph will work with a state shaped like `ConversationState`.”*
* `add_node("get_input", user_input_node)` – registers a **node name** and maps it to the actual function.
* `add_node("generate_response", chatbot_node)` – same concept for the chatbot node.

At this stage you’ve created the **building blocks** of a workflow, but you haven’t yet defined how the blocks connect.

---

### Step 5 – Define Edges (Flow of Execution)

```python
# Step 5: Define edges
builder.set_entry_point("get_input")
builder.add_edge("get_input", "generate_response")
builder.add_edge("generate_response", END)
```

**What to do**

1. Paste into a **new cell**.
2. Run the cell.

**What you should see**

* No visible output, but the graph structure is now fully wired.

**Explanation**

* `set_entry_point("get_input")`

  * Tells LangGraph: **this is where execution starts**.
  * When you call `graph.invoke`, it will first run the `"get_input"` node.
* `add_edge("get_input", "generate_response")`

  * After `"get_input"` finishes, LangGraph **automatically moves** to `"generate_response"`.
* `add_edge("generate_response", END)`

  * After the chatbot node finishes, the graph reaches `END`, meaning execution stops and returns the final state.

The full flow is:

`ENTRY → get_input → generate_response → END`

This is the **control flow** of your chatbot workflow.

---

### Step 6 – Compile and Run the Graph

```python
# Step 6: Compile and run
graph = builder.compile()
result = graph.invoke({"messages": []})
print("Chatbot Response:", result["response"])


graph = builder.compile()
result = graph.invoke({"messages": []})
print("Chatbot Response:", result["response"])
```

**What to do**

1. Paste this block into a **new cell**.
2. Run it.

**What you should see**

* The graph is compiled and executed **twice**.
* You should see two lines printed, for example:

  ```text
  Chatbot Response: Malaysia is a country located in Southeast Asia...
  Chatbot Response: Malaysia is a country in Southeast Asia, divided into Peninsular...
  ```

  (Exact wording may differ, but both responses should correctly describe where Malaysia is.)

**Explanation**

* `graph = builder.compile()`

  * Converts your `StateGraph` definition into a **runnable graph object**.

* `graph.invoke({"messages": []})`

  * Starts the graph with an initial state that only contains `messages: []` (an empty list).
  * Internally, the graph:

    1. Runs `"get_input"` → adds `"user_input": "Tell me where is Malaysia?"`.
    2. Runs `"generate_response"` → calls OpenAI, gets a reply, and updates `response` and `messages`.
    3. Hits `END` → returns the final state as `result`.

* `print("Chatbot Response:", result["response"])`

  * Displays the LLM’s answer stored in the `response` key.

* The code compiles and runs the graph **twice**:

  * This lets you see that each run sends the same question but might produce slightly different wording (typical LLM behavior), even though the meaning stays the same.

> Good discussion point for class:
>
> * *“Even with the same prompt, LLMs can respond with slightly different phrasing. Is that acceptable for your use case? When do you need deterministic behavior?”*


## Lab: Simple Chatbot with Memory (Debug-Friendly Version)

---

## Learning Objectives

By the end of this lab, participants will be able to:

1. Explain the purpose of wrapping a LangGraph workflow inside a Python function.
2. Define and use a simple state type (`SimpleState`) to track messages and current input.
3. Implement a **user input node** that simulates user turns based on the current conversation round.
4. Implement a **response node** that:

   * Builds a prompt from previous messages (memory)
   * Sends it to an LLM using `ChatOpenAI`
   * Updates the conversation history (`messages`)
5. Build a minimal LangGraph with:

   * Two nodes (`get_input`, `respond`)
   * A conditional edge to stop after a fixed number of messages.
6. Use `graph.stream()` to trace execution step-by-step and print user and assistant messages as they are generated.
7. Observe and explain the difference between the **graph’s internal state** and the local `state` variable printed at the end (for debugging and understanding state flow).

---

## Step-by-Step Lab Guide (with Explanation)

> Participants should **reuse the imports from earlier labs** (`TypedDict`, `List`, `Dict`, `StateGraph`, `ChatOpenAI`) and then add this function exactly as given.

---

### Step 1 – Define a Simple Conversation Function

```python
# Simplified version that's easier to debug
def simple_conversation_with_memory():
    print("=== Simple Chatbot with Memory ===\n")
```

**What to do**

1. Paste the entire `simple_conversation_with_memory` function (all the code you provided) into a new cell.
2. Make sure all previous imports (`TypedDict`, `List`, `Dict`, `StateGraph`, `ChatOpenAI`) are already in the notebook.
3. Run the cell to define the function (no output yet except potential syntax errors).

**Explanation**

* Wrapping everything inside `simple_conversation_with_memory()`:

  * Keeps this example **self-contained**.
  * Makes it easy to re-run the entire mini-demo by calling the function again.
* The first `print` is a simple header so learners can see when this lab’s conversation starts.

---

### Step 2 – Define the Simple State Structure

Inside the function:

```python
    # Build a simpler graph for demonstration
    class SimpleState(TypedDict):
        messages: List[Dict[str, str]]
        current_input: str
```

**What to do**

* Read this section inside the function and understand what it represents.

**Explanation**

* `SimpleState` is a `TypedDict` that describes the shape of the graph state:

  * `messages`: a list of `{ "role": ..., "content": ... }` dictionaries, holding the full conversation history.
  * `current_input`: the current user input for this step.
* Compared to the previous lab, this state is **minimal on purpose**:

  * No extra fields like `response` or `conversation_context`.
  * Focus is only on the evolving `messages` list and the latest `current_input`.

This simplification makes it easier for learners to reason about state.

---

### Step 3 – Create the User Input Node

```python
    def get_user_input(state: SimpleState):
        # Simulate user input
        inputs = ["Hello!", "What's your name?", "What can you help me with?"]
        round_num = len(state["messages"]) // 2
        current_input = inputs[min(round_num, len(inputs)-1)]
        return {"current_input": current_input}
```

**What to do**

* Walk through this logic and run the whole function later to see these simulated inputs printed.

**Explanation**

* `inputs`: a fixed list of possible user messages to simulate a small conversation.
* `round_num = len(state["messages"]) // 2`:

  * Each full round is 2 messages (one user, one assistant).
  * `len(state["messages"])` counts how many messages have already happened.
  * Dividing by 2 gives the **current conversation round index**.
* `min(round_num, len(inputs)-1)`:

  * Ensures we don’t go out of bounds if there are more rounds than predefined inputs.
  * After the last input, it keeps reusing the final prompt.
* Returns a dictionary with `"current_input"`:

  * LangGraph will merge this into the graph state for subsequent nodes.

This node **simulates user input** without needing real-time user interaction.

---

### Step 4 – Create the Response Node with Memory

```python
    def generate_response(state: SimpleState):
        llm = ChatOpenAI(model="gpt-3.5-turbo")

        # Build context from all previous messages
        context = "\n".join([f"{m['role']}: {m['content']}" for m in state["messages"]])
        prompt = f"""Previous conversation:
{context}

User: {state['current_input']}

Assistant:"""

        response = llm.invoke(prompt)

        # Update messages
        new_messages = state["messages"] + [
            {"role": "user", "content": state["current_input"]},
            {"role": "assistant", "content": response.content}
        ]

        return {"messages": new_messages}
```

**What to do**

* Understand how the **conversation memory** is constructed and used in the prompt.
* After running the full function later, observe the assistant’s answers and how they reflect previous context.

**Explanation**

* `llm = ChatOpenAI(model="gpt-3.5-turbo")`:

  * Creates an LLM client that can be called using `.invoke()`.
* `context = "\n".join([...])`:

  * Loops through all previous `messages` in the state.
  * Formats them as `role: content` lines.
  * Joins them into a single string to feed into the prompt.
* `prompt = f"""Previous conversation: ... """`:

  * Includes the entire previous conversation as context.
  * Adds the **current user input**.
  * Ends with `Assistant:` to encourage the model to continue the dialog.
* `response = llm.invoke(prompt)`:

  * Sends the prompt to the model and gets a response object.
* `new_messages`:

  * Appends the new user message and assistant message to the existing `messages`.
* Returns a dict with `"messages": new_messages`:

  * This updated history becomes the new state for further steps.

This node shows a classic **prompt-based memory** pattern: feed back previous messages on every turn.

---

### Step 5 – Build the Simple Graph

```python
    # Build graph
    simple_builder = StateGraph(SimpleState)
    simple_builder.add_node("get_input", get_user_input)
    simple_builder.add_node("respond", generate_response)

    simple_builder.set_entry_point("get_input")
    simple_builder.add_edge("get_input", "respond")
    simple_builder.add_conditional_edges(
        "respond",
        lambda state: "end" if len(state["messages"]) >= 6 else "get_input"
    )

    simple_graph = simple_builder.compile()
```

**What to do**

* Note how few nodes and edges this graph uses — it is intentionally minimal for debugging.

**Explanation**

* `simple_builder = StateGraph(SimpleState)`:

  * Tells LangGraph what the state type is.
* `add_node("get_input", get_user_input)` and `add_node("respond", generate_response)`:

  * Registers the two functions as nodes with names `"get_input"` and `"respond"`.
* `set_entry_point("get_input")`:

  * The graph will always start by calling `get_input`.
* `add_edge("get_input", "respond")`:

  * After getting the user input, the graph immediately transitions to the response node.
* `add_conditional_edges(...)`:

  * The lambda checks the length of `state["messages"]`.
  * If there are **6 or more messages**, it returns `"end"`; otherwise it returns `"get_input"`.
  * This means:

    * A conversation can contain up to 3 rounds (user + assistant = 2 messages per round → 3 rounds → 6 messages).
* `simple_graph = simple_builder.compile()`:

  * Converts the builder into an executable graph object.

This is a **minimal conversation loop**: input → respond → decide to continue or end.

---

### Step 6 – Run the Conversation with Streaming

```python
    # Run conversation
    state = {"messages": [], "current_input": ""}

    print("Starting conversation...")
    for i, step in enumerate(simple_graph.stream(state)):
        for node, output in step.items():
            if node == "get_input":
                print(f"\nRound {(i//2)+1}:")
                print(f"User: {output['current_input']}")
            elif node == "respond":
                last_message = output['messages'][-1]
                if last_message['role'] == 'assistant':
                    print(f"Assistant: {last_message['content']}")

    print(f"\nFinal memory: {len(state['messages'])} messages")
```

**What to do**

1. At the bottom of the cell (outside the function), call:

   ```python
   # Run the simple version
   simple_conversation_with_memory()
   ```
2. Run the cell and observe the printed conversation.

**What you should see**

* A header:
  `=== Simple Chatbot with Memory ===`
* `Starting conversation...`
* Then, per round:

  * `Round 1:` then `User: Hello!` and an assistant reply
  * `Round 2:` then `User: What's your name?` and a reply
  * `Round 3:` then `User: What can you help me with?` and a reply
* Finally, a line: `Final memory: 0 messages` (since the local `state` dict itself is not being updated in-place by the graph).

**Explanation**

* `state = {"messages": [], "current_input": ""}`:

  * Initial state is empty — no previous messages, no current input.
* `simple_graph.stream(state)`:

  * Yields each **step** of the graph execution.
  * Each `step` is a mapping of `{node_name: node_output_state_fragment}`.
* `if node == "get_input"`:

  * Prints the current round number and the simulated user input.
* `elif node == "respond"`:

  * Takes the last message in `output["messages"]`.
  * If the last message is from the assistant, prints the assistant’s reply.
* `Final memory: {len(state['messages'])} messages`:

  * This prints the length of the original `state["messages"]` dict passed in.
  * The compiled graph manages its own internal state and returns updates via `step` outputs rather than mutating the original Python object directly.
  * This is useful to discuss how LangGraph treats state: **immutable-style updates** (returning new state) instead of in-place changes.

You can use this as a debugging and teaching moment:

* *“We see a full conversation printed, but `state["messages"]` is still empty. Why? Because the graph framework returns new state instead of modifying the `state` variable we created in Python.”*

---

### Step 7 – Run the Function

At the very end of the code:

```python
# Run the simple version
simple_conversation_with_memory()
```

**What to do**

* Make sure this line is present after the function definition.
* Run the cell and watch the conversation play out.

**Explanation**

* This line simply invokes the demo.
* You can comment/uncomment this line to control when the lab runs, without redefining the function every time

## Lab: Multi-Agent Research Workflow with LangGraph and ChatOpenAI

---

## Learning Objectives

By the end of this lab, participants will be able to:

1. Describe the idea of a **multi-agent workflow** where different “agents” handle different stages of a task (research, analysis, report).
2. Define a **state structure** (`ResearchState`) that carries data through multiple nodes in a LangGraph.
3. Implement separate agent functions that:

   * Simulate research on a given topic
   * Use an LLM to analyze research material
   * Use an LLM to generate a final report
4. Build a LangGraph with:

   * Multiple nodes (`research`, `analyze`, `report`)
   * Conditional routing logic between nodes.
5. Execute the graph with an initial state and inspect the **final report** produced by the workflow.
6. Explain the **data flow** across the pipeline: topic → research material → analysis → report → completion.

---

## Step-by-Step Lab Guide (with Explanation)

> Assumption: Participants already have `StateGraph`, `END`, `ChatOpenAI`, `Dict`, `Any` imported and an OpenAI API key configured from previous labs.

---

### Step 0 – Make Sure Prerequisites Are Ready

Before adding this lab code, ensure you already have in your notebook:

* `from langgraph.graph import StateGraph, END`
* `from typing import Dict, Any`
* `from langchain_openai import ChatOpenAI`
* `os.environ["OPENAI_API_KEY"] = "your-api-key"` set somewhere above.

You **do not** need to modify the lab code itself.

---

### Step 1 – Define the State and Agent Nodes

```python
# Step 1: Define state and nodes
class ResearchState(Dict[str, Any]):
    topic: str
    research_material: str
    analysis: str
    report: str
    current_step: str

def research_agent(state: ResearchState):
    # Simulate research
    return {"research_material": f"Research on {state['topic']}: ...", "current_step": "analyze"}

def analysis_agent(state: ResearchState):
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    analysis = llm.invoke(f"Analyze this research: {state['research_material']}")
    return {"analysis": analysis.content, "current_step": "report"}

def report_agent(state: ResearchState):
    llm = ChatOpenAI(model="gpt-3.5-turbo")
    report = llm.invoke(f"Create report from analysis: {state['analysis']}")
    return {"report": report.content, "current_step": "complete"}
```

**What to do**

1. Paste this block into a new cell in your notebook.
2. Run the cell to register `ResearchState` and the three agent functions.

**Explanation**

* `class ResearchState(Dict[str, Any])`

  * Defines the **shape of the state** the graph will pass around.
  * Keys (conceptually):

    * `topic`: the research topic (string).
    * `research_material`: text representing the raw research output.
    * `analysis`: structured or narrative analysis of the research.
    * `report`: final report text.
    * `current_step`: a simple label showing what stage the process is at (e.g., `"analyze"`, `"report"`, `"complete"`).

* `research_agent(state: ResearchState)`

  * Reads `state['topic']`.
  * Returns a dictionary with:

    * `research_material`: a simulated research string containing the topic.
    * `current_step`: set to `"analyze"` to indicate the next logical stage.
  * In a real system, this node could call APIs, search the web, or query a database. In this lab, we keep it **simple and deterministic**.

* `analysis_agent(state: ResearchState)`

  * Creates an LLM client with `ChatOpenAI(model="gpt-3.5-turbo")`.
  * Calls `.invoke()` with a prompt: `"Analyze this research: <research_material>"`.
  * The LLM’s response is stored in `analysis.content`.
  * Returns:

    * `analysis`: the LLM-generated analysis text.
    * `current_step`: `"report"` to indicate the next stage in the workflow.

* `report_agent(state: ResearchState)`

  * Also uses `ChatOpenAI`.
  * Prompts the model with: `"Create report from analysis: <analysis>"`.
  * The LLM returns a report based on the previous analysis.
  * Returns:

    * `report`: the final, more polished report content.
    * `current_step`: `"complete"` to mark the workflow as done.

These three agents form a **pipeline**:

> Topic → Research → Analysis → Report

---

### Step 2 – Build the Graph and Routing Logic

```python
# Step 2: Build graph with conditional edges
def route_after_research(state: ResearchState):
    return "analyze"

def route_after_analysis(state: ResearchState):
    return "report"

builder = StateGraph(ResearchState)
builder.add_node("research", research_agent)
builder.add_node("analyze", analysis_agent)
builder.add_node("report", report_agent)

builder.set_entry_point("research")
builder.add_conditional_edges("research", route_after_research)
builder.add_edge("analyze", "report")
builder.add_edge("report", END)
```

**What to do**

1. Paste this block into the next cell and run it.

**Explanation**

* `route_after_research(state)`:

  * A small routing function that always returns `"analyze"`.
  * In more advanced scenarios, this function could inspect `state` and choose different next steps based on conditions (e.g., quality, confidence, flags).

* `route_after_analysis(state)` (defined but not used in this exact snippet):

  * Returns `"report"` — showing how you could define routing logic after analysis as well.

* `builder = StateGraph(ResearchState)`:

  * Creates a graph builder that expects its state to look like `ResearchState`.

* `builder.add_node("research", research_agent)`

* `builder.add_node("analyze", analysis_agent)`

* `builder.add_node("report", report_agent)`

  * Registers each node with a name and its associated function.

* `builder.set_entry_point("research")`:

  * Execution will **begin** at the `"research"` node.

* `builder.add_conditional_edges("research", route_after_research)`:

  * After the `"research"` node finishes, LangGraph calls `route_after_research(state)` to determine where to go next.
  * Since this function always returns `"analyze"`, the next node will be `"analyze"`.

* `builder.add_edge("analyze", "report")`:

  * After `"analyze"` completes, the graph automatically moves to `"report"`.

* `builder.add_edge("report", END)`:

  * After `"report"` completes, the graph reaches `END` and stops.
  * The final `state` at this point contains the `report`.

Overall control flow:

> `research` → (route) → `analyze` → `report` → END

---

### Step 3 – Run the Multi-Agent System and View the Report

```python
# Step 3: Run the multi-agent system
graph = builder.compile()
result = graph.invoke({"topic": "Artificial Intelligence in Healthcare"})
print("Final Report:", result["report"])
```

**What to do**

1. Paste this block into a new cell.
2. Run the cell to execute the full workflow.

**What you should see**

* The code compiles the graph and then runs it once with the initial state:

  ```python
  {"topic": "Artificial Intelligence in Healthcare"}
  ```
* At the end, it prints something like:

  ```text
  Final Report: <a nicely written paragraph or multi-paragraph report about AI in healthcare>
  ```

  (The exact wording will vary depending on the LLM response.)

**Explanation**

* `graph = builder.compile()`:

  * Converts the builder definition into a runnable graph object.
  * All nodes, edges, and routing logic are now fixed and ready for execution.

* `graph.invoke({"topic": "Artificial Intelligence in Healthcare"})`:

  * Starts execution with an initial state containing only the `topic`.
  * Under the hood:

    1. Runs `"research"`:

       * Uses `topic` to create `research_material`.
    2. Routing points to `"analyze"`:

       * Uses `research_material` in a prompt to the LLM.
       * Stores the analysis text in `analysis`.
    3. `"analyze"` transitions to `"report"`:

       * Uses `analysis` in a second LLM call to generate a full report.
    4. `"report"` transitions to `END`:

       * Returns the final state, which now includes `report`.

* `result["report"]`:

  * Accesses the final report text from the returned state dictionary.

This showcases a simple but powerful pattern:

* **Multi-step AI workflow**
* Each step adds or transforms information in the shared **state**
* The final node outputs a deliverable (here: a textual report).


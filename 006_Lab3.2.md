Here we go, Truman ‚Äî turning this monster into a clear lab handout without touching the code.

---

## Lab Title

**Lab: Enhanced Multi-Format Document Processing Pipeline with LangGraph**

---

## Learning Objectives

By the end of this lab, participants will be able to:

1. Install and import all required libraries to build a document-processing pipeline in Google Colab (PDF, DOCX, TXT, XLSX support).
2. Explain the purpose of the `DocumentState` structure and how it tracks both content (text, summary, keywords) and metadata (file name, type, word count, page count, errors).
3. Upload documents in different formats and extract text using appropriate libraries (`pdfplumber`, `PyPDF2`, `python-docx`, `openpyxl`).
4. Implement validation and cleaning steps to:

   * Enforce length limits
   * Count words
   * Remove noisy artifacts while preserving structure
5. Use `ChatOpenAI` to:

   * Generate a structured, detailed summary of the document
   * Extract meaningful keywords, with a non-LLM fallback if needed
6. Build a LangGraph pipeline with:

   * Validation, cleaning, summarization, keyword extraction, and success/error handlers
   * Conditional routing for error handling
7. Run the pipeline in two modes:

   * Upload & process a real document
   * Process a built-in sample report
     and interpret the printed results (summary, keywords, statistics, and status).

---

## Step-by-Step Lab Guide (with Explanation)

> Assumption: You are running this in **Google Colab** (because of `google.colab.files.upload()`).

---

### Step 1 ‚Äì Install Dependencies and Import Modules

**Code already provided:**

```python
# Step 1: Install required dependencies (fixed)
!pip install langgraph langchain-openai python-docx pdfplumber openpyxl PyPDF2

import os
import tempfile
from google.colab import files
from langgraph.graph import StateGraph, END
from typing import Dict, Any, List, TypedDict
from langchain_openai import ChatOpenAI

# Document processing imports (fixed)
import pdfplumber
import PyPDF2
from docx import Document
import openpyxl
import io

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = "sk-proj-..."
```

**What to do**

1. Put this whole block in the **first cell** of a new Colab notebook.
2. Run the cell so that:

   * Required libraries are installed.
   * All imports are available.
   * The OpenAI API key environment variable is set (the code already contains one, but in a real class they should use their own key).

**Explanation**

* `!pip install ...` ensures all needed libraries are available in the Colab environment.
* `google.colab.files` is used for interactive file upload.
* `pdfplumber`, `PyPDF2`, `Document` (from `python-docx`), and `openpyxl` provide **format-specific** text extraction:

  * PDFs: `pdfplumber` (primary), `PyPDF2` (fallback).
  * DOCX: paragraphs + tables via `Document`.
  * XLSX/XLS: cell text via `openpyxl`.
  * TXT: handled via decoding.
* `StateGraph, END` are for building the LangGraph workflow.
* `ChatOpenAI` is the LLM interface used for summarization and keyword extraction.

---

### Step 2 ‚Äì Define the Document State Structure

**Code:**

```python
# Step 2: Enhanced Document Processing Pipeline
class DocumentState(TypedDict):
    raw_document: str
    cleaned_text: str
    summary: str
    keywords: List[str]
    processed: bool
    error: str
    file_name: str
    file_type: str
    word_count: int
    page_count: int  # For PDFs
```

**What to do**

1. Keep this directly after Step 1 code.
2. Run the cell (or keep it in the same cell as Step 1 if you prefer).

**Explanation**

`DocumentState` defines everything the pipeline cares about:

* **Content fields**

  * `raw_document`: extracted text from the file.
  * `cleaned_text`: cleaned and normalized text ready for LLM use.
  * `summary`: LLM-generated summary.
  * `keywords`: a list of keywords/key phrases.
* **Status fields**

  * `processed`: whether the pipeline completed successfully.
  * `error`: error message if something goes wrong.
* **Metadata fields**

  * `file_name`: original file name.
  * `file_type`: human-friendly type (PDF/DOCX/TXT/XLSX).
  * `word_count`: number of words in `raw_document`.
  * `page_count`: number of pages (mainly for PDFs).

This acts as the **single source of truth** that flows through the graph.

---

### Step 3 ‚Äì File Upload and Text Extraction

**Code (functions):**

```python
# Step 3: File Upload and Processing Functions (Fixed)
def upload_document():
    """Upload document to Google Colab"""
    ...
    return file_name, file_content

def extract_text_from_file(file_name: str, file_content: bytes) -> tuple[str, str, int]:
    """Extract text from various file formats with improved PDF handling"""
    ...
    return text.strip(), error, page_count

def process_uploaded_document():
    """Main function to handle document upload and processing"""
    ...
    return extracted_text, file_name, file_type, page_count
```

**What to do**

1. Paste all three functions exactly as provided below Step 2.
2. Run the cell so these helper functions are defined.

**Explanation ‚Äì `upload_document()`**

* Prints an instruction: *‚ÄúPlease upload a document (PDF, DOCX, TXT, or XLSX):‚Äù*
* Uses `files.upload()` (Google Colab UI) to let the user choose a file.
* Returns:

  * `file_name`: name of the uploaded file.
  * `file_content`: raw bytes of the file.

**Explanation ‚Äì `extract_text_from_file(file_name, file_content)`**

* Writes the file to a **temporary file** on disk so the other libraries can read it.

* Determines the file extension and chooses extraction logic:

  * **PDF (`.pdf`)**

    1. Tries `pdfplumber`:

       * Counts pages, loops over each, extracts text.
       * Adds page markers like `--- Page 1 ---`.
    2. If that fails or returns no text, falls back to `PyPDF2`:

       * Again, loops pages and extracts text.
    3. Keeps track of any errors.

  * **DOCX (`.docx`)**

    * Reads paragraphs and appends non-empty ones.
    * Also extracts text from tables (row by row).

  * **Excel (`.xlsx`, `.xls`)**

    * Loads the workbook with `openpyxl`.
    * Loops sheet by sheet, row by row, concatenating cell values with `" | "`.
    * Adds sheet headers like `=== SheetName ===`.

  * **Text (`.txt`)**

    * Tries decoding with several common encodings (`utf-8`, `latin-1`, `cp1252`).
    * Reports if decoding fails.

  * **Others**

    * Marks unsupported file types with an error.

* Cleans up the temporary file.

* Returns:

  * Extracted text (string, possibly empty).
  * Error message (or `None` / partial warning).
  * Page count (for PDFs; 0 otherwise).

**Explanation ‚Äì `process_uploaded_document()`**

* Calls `upload_document()` to get a file.
* Calls `extract_text_from_file(...)` to get text, error, and page count.
* Prints:

  * File name
  * File size (bytes)
  * Character count extracted
  * Page count and any warning about minor errors
* Determines `file_type` from the extension.
* Returns `(extracted_text, file_name, file_type, page_count)` or an appropriate error if extraction fails.

This step encapsulates **I/O and parsing** so the rest of the pipeline works on pure text and metadata.

---

### Step 4 ‚Äì Validation, Cleaning, Summarization, Keyword Extraction

**Code (nodes):**

```python
# Step 4: Enhanced Processing Nodes
def validate_input(state: DocumentState):
    ...

def clean_document(state: DocumentState):
    ...

def summarize_document(state: DocumentState):
    ...

def extract_keywords(state: DocumentState):
    ...
```

**What to do**

1. Paste these four functions under Step 3.
2. Run the cell.

**Explanation ‚Äì `validate_input(state)`**

* Checks:

  * If there is any `raw_document` content.
  * If length ‚â• 10 characters (otherwise error: ‚ÄúDocument too short‚Ä¶‚Äù).
  * If length ‚â§ 50,000 characters (otherwise error describing how large it is).
* Computes `word_count` from `raw_document`.
* Returns:

  * `error`: `None` if OK, or error message.
  * `word_count`
  * `processed`: `False` (still in progress).
* Purpose: prevent:

  * Empty uploads.
  * Extremely short content (useless for summarization).
  * Extremely long content that might exceed LLM context.

**Explanation ‚Äì `clean_document(state)`**

* Splits the document into lines.
* For each line:

  * Strips edges, skips empty lines.
  * Normalizes multiple internal spaces to single spaces.
* Joins cleaned lines with `\n` to keep paragraph breaks.
* Replaces common artifacts:

  * Null chars (`\x00`), BOM (`\ufeff`), normalize line endings, collapse double spaces.
* If cleaned text > 40,000 chars:

  * Truncates and appends a notice about truncation.
* Returns `{"cleaned_text": cleaned}`.
* Goal: produce a **clean, reasonably sized** text for LLM processing.

**Explanation ‚Äì `summarize_document(state)`**

* Creates `llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)`.
* Builds a detailed prompt containing:

  * File metadata: name, type, pages, word count.
  * Cleaned document content.
  * Clear instructions to return:

    1. Main topic
    2. Key points
    3. Purpose
    4. Key data
    5. Overview
* Calls `llm.invoke(prompt)` inside a `try` block.
* Returns `{"summary": summary.content}` on success, or sets an `error` if summarization fails.
* This produces a **structured, multi-section summary**.

**Explanation ‚Äì `extract_keywords(state)`**

* Uses `ChatOpenAI` again with lower temperature (0.2).
* Builds a prompt using the summary (not the raw text) and asks for a **comma-separated list** of 5‚Äì7 keywords.
* Parses the response:

  * Splits by comma, strips whitespace.
  * Filters out empty/1-character entries.
* If no keywords are produced but there is cleaned text:

  * Uses a **fallback** method:

    * Counts word frequencies of longer alphabetic words.
    * Picks the most common ones as keywords.
* Returns:

  * `"keywords"`: list of cleaned keywords.
  * `"processed": True` on success.
* If anything fails, returns an `error` and `processed: False`.

This step outputs **high-level tags** for the document.

---

### Step 5 ‚Äì Routing, Error Handler, and Success Handler

**Code:**

```python
# Step 5: Enhanced Routing and Error Handling
def route_after_validation(state: DocumentState):
    ...

def error_handler(state: DocumentState):
    ...

def success_handler(state: DocumentState):
    ...
```

**What to do**

1. Paste these functions below Step 4.
2. Run the cell.

**Explanation**

* `route_after_validation(state)`:

  * Checks `state.get("error")`.
  * If there is an error ‚Üí returns `"error_handler"`.
  * Otherwise ‚Üí returns `"clean"`.
  * This function decides the **branch** after validation.

* `error_handler(state)`:

  * Prints an error message with a ‚ùå icon and the error text.
  * Returns `{"processed": False}`.
  * Central place to mark pipeline as failed.

* `success_handler(state)`:

  * Prints a success message with ‚úÖ.
  * Returns `{"processed": True}`.
  * Final node run after successful keyword extraction.

---

### Step 6 ‚Äì Build and Compile the Enhanced Graph

**Code:**

```python
# Step 6: Build the Enhanced Graph
builder = StateGraph(DocumentState)

# Add nodes
builder.add_node("validate", validate_input)
builder.add_node("clean", clean_document)
builder.add_node("summarize", summarize_document)
builder.add_node("extract_keywords", extract_keywords)
builder.add_node("error_handler", error_handler)
builder.add_node("success_handler", success_handler)

# Define workflow
builder.set_entry_point("validate")
builder.add_conditional_edges("validate", route_after_validation)
builder.add_edge("clean", "summarize")
builder.add_edge("summarize", "extract_keywords")
builder.add_edge("extract_keywords", "success_handler")
builder.add_edge("success_handler", END)
builder.add_edge("error_handler", END)

# Compile the graph
graph = builder.compile()
```

**What to do**

1. Paste this block below Step 5.
2. Run the cell.

**Explanation**

* Nodes registered:

  * `"validate"` ‚Üí `validate_input`
  * `"clean"` ‚Üí `clean_document`
  * `"summarize"` ‚Üí `summarize_document`
  * `"extract_keywords"` ‚Üí `extract_keywords`
  * `"error_handler"` ‚Üí `error_handler`
  * `"success_handler"` ‚Üí `success_handler`
* Entry point: `"validate"`.
* Flow:

  * After `"validate"`, `route_after_validation` chooses:

    * `"clean"` (normal path), or
    * `"error_handler"` (failure path).
  * `"clean"` ‚Üí `"summarize"` ‚Üí `"extract_keywords"` ‚Üí `"success_handler"` ‚Üí `END`.
  * `"error_handler"` ‚Üí `END`.
* `graph = builder.compile()` makes an executable pipeline from this definition.

---

### Step 7 ‚Äì Main Execution: Upload and Process a Document

**Code:**

```python
# Step 7: Main Execution Function
def process_document_pipeline():
    """Complete document processing pipeline"""
    ...
    return result
```

**What to do**

1. Paste the full `process_document_pipeline()` function exactly as provided.
2. Run the cell.

**Explanation**

* Prints a header: *‚ÄúDocument Processing Pipeline‚Äù*.
* Calls `process_uploaded_document()`:

  * Prompts the user to upload a file.
  * Extracts text, file name, type, and page count.
* Constructs `initial_state` of type `DocumentState`:

  * Fills in `raw_document`, file metadata, and default values for other fields.
* Calls `graph.invoke(initial_state)`:

  * Runs the full LangGraph pipeline you built.
* Prints a nice **result section**:

  * File name, type, pages, word count.
  * Processing status: success or failed.
  * If success:

    * Shows formatted summary.
    * Lists keywords with numbering.
  * If failure:

    * Shows the error message.

To run it in Colab (without changing the code above), you can add a **new cell**:

```python
process_document_pipeline()
```

and execute that cell.

---

### Step 8 ‚Äì Alternative: Process a Built-In Sample Document

**Code:**

```python
# Step 8: Alternative - Process Sample Document
def process_sample_document():
    """Process a sample document without upload"""
    ...
    return result
```

**What to do**

1. Paste the full `process_sample_document()` function below Step 7.
2. Run the cell.

**Explanation**

* Uses a built-in multi-section report about **AI in healthcare** as `sample_doc`.
* Sets up `initial_state` with:

  * `raw_document` = sample text.
  * `file_name` = `"sample_ai_healthcare_report.pdf"`.
  * `file_type` = `"PDF"`.
  * `page_count` = `3` (simulated).
* Calls `graph.invoke(initial_state)` using the same pipeline.
* Prints:

  * Processing status.
  * Full summary.
  * Comma-joined keywords.

To try it, create a new cell and run:

```python
process_sample_document()
```

This is useful for testing the pipeline **without uploading** any file.

---

### Step 9 ‚Äì Usage Options / How to Run in Class

At the end of the file, the code gives you hints:

```python
# Step 9: Usage Examples
print("üöÄ Enhanced Document Processing Pipeline Ready!")
print("\nChoose an option:")
print("1. process_document_pipeline() - Upload and process your own document")
print("2. process_sample_document() - Process the built-in sample document")

# Uncomment the line below to test with sample document
# process_sample_document()

# Uncomment the line below to upload and process your own document
# process_document_pipeline()
```

**What to do (without changing existing code)**

* Keep this as-is to show the options.
* In a **new cell**, let learners choose what to run, for example:

```python
# Option A: test with sample document
process_sample_document()

# or

# Option B: upload a document (run this instead)
# process_document_pipeline()
```

**Explanation**

* This lets you demo both flows:

  * **Sample mode** ‚Äì deterministic, good for projector demo.
  * **Upload mode** ‚Äì interactive, lets participants bring their own PDFs, DOCX, or Excel reports.

## Lab 4: Customer Support Intent Router with LangGraph

---

## Learning Objectives

By the end of this lab, participants will be able to:

1. Explain the role of the `SupportState` structure in managing a multi-step customer support workflow (intent, department, escalation, history, resolution).
2. Use `ChatOpenAI` to classify customer intents (billing, technical, sales, general) based on conversation context.
3. Implement a routing function that directs queries to the correct ‚Äúdepartment‚Äù node, with escalation to a human agent when needed.
4. Build specialized response handlers for different departments (billing, technical, sales, general) that:

   * Use conversation history
   * Generate focused, empathetic replies
5. Implement escalation logic for human agents and a safe conversation-ending flow using LangGraph conditional edges.
6. Build and compile a LangGraph graph that:

   * Starts with intent classification
   * Routes to department nodes
   * Optionally triggers a closing message node
7. Run and test the system in two modes:

   * **Interactive chat** using `run_interactive_support()`
   * **Automated test runs** using `test_support_system()`

---

## Step-by-Step Lab Guide (with Explanation)

> Learners should **copy your code exactly** into a notebook/IDE and only add calls like `run_interactive_support()` or `test_support_system()` in separate cells/lines when they want to execute.

---

### Step 1 ‚Äì Install and Import Dependencies

Your code:

```python
# Step 1: Install and import dependencies
#!pip install langgraph langchain-openai

import os
from langgraph.graph import StateGraph, END
from typing import Dict, Any, TypedDict, List
from langchain_openai import ChatOpenAI

# Set your OpenAI API key
#os.environ["OPENAI_API_KEY"] = "your-api-key-here"
```

**What learners do**

1. If running in Colab or a fresh environment, uncomment and run the `pip install` line once.
2. Set their own API key in the commented line (if they want real LLM responses).
3. Run this cell so imports are available.

**Explanation**

* `StateGraph`, `END`: used to build and terminate the LangGraph workflow.
* `TypedDict`, `List`, `Dict`: define structured state types.
* `ChatOpenAI`: an LLM client for intent classification and department responses.
* The API key is required to talk to OpenAI; you keep it externalized to keep code clean.

---

### Step 2 ‚Äì Define the Support State

Your code:

```python
# Step 2: Enhanced Support State
class SupportState(TypedDict):
    user_query: str
    intent: str
    department: str
    response: str
    escalated: bool
    conversation_history: List[Dict[str, str]]
    needs_human: bool
    resolution_status: str
    should_continue: bool  # New field to control conversation flow
```

**What learners do**

* Run this cell (or keep it in the same cell as Step 1) to define `SupportState`.

**Explanation**

`SupportState` tracks everything about the support interaction:

* `user_query`: current user message.
* `intent`: classified category (billing/technical/sales/general).
* `department`: which ‚Äúteam‚Äù handled the current response.
* `response`: the latest assistant reply.
* `escalated`: whether the issue has been escalated to a human.
* `conversation_history`: list of `{role, content}` messages.
* `needs_human`: flag for human escalation.
* `resolution_status`: e.g. `"pending"`, `"escalated"`, `"resolved"`.
* `should_continue`: controls whether the conversation should keep going.

This is the **shared memory** passed through all nodes in the graph.

---

### Step 3 ‚Äì Interactive User Input

Your code:

```python
# Step 3: Interactive Input Function
def get_user_input():
    ...
    return user_input, False / True
```

**What learners do**

* Read and understand this function; they don‚Äôt call it directly ‚Äî it is used by `run_interactive_support()`.

**Explanation**

* Prints a header and prompt to the console.
* Accepts user input from `input("You: ")`.
* Special handling:

  * `exit` ‚Üí end the chat.
  * `help` ‚Üí prints example issues and returns `"help"` without ending.
  * ‚Äúthanks/thank you/that‚Äôs all/no more questions‚Äù ‚Üí end the conversation gracefully.
* Returns:

  * First value: the actual text or special tokens (`None`, `"help"`).
  * Second value: whether the chat loop should end.

This is your **front-door** into the support system.

---

### Step 4 ‚Äì Intent Classification Node

Your code:

```python
# Step 4: Enhanced Intent Classification
def classify_intent(state: SupportState):
    ...
    return {"intent": intent}
```

**What learners do**

* Run the cell defining `classify_intent`.

**Explanation**

* Uses `ChatOpenAI` to:

  * Look at the current query (`state['user_query']`).
  * Optionally look at **recent conversation history** (last 4 messages).
  * Classify the query into one of:

    * `billing`
    * `technical`
    * `sales`
    * `general`

* Prompt explicitly instructs the LLM to respond with **only** the category name.

* If the classification output is unexpected, default to `"general"`.

This is the **entry node** of the graph and decides routing.

---

### Step 5 ‚Äì Department Routing Logic

Your code:

```python
# Step 5: Department Routing
def route_to_department(state: SupportState):
    ...
    return departments.get(intent, "general_support")
```

**What learners do**

* Run this function definition.

**Explanation**

* First, checks for **escalation keywords** in `user_query`:

  * e.g., `"manager"`, `"supervisor"`, `"escalate"`, `"complaint"`, `"angry"`, `"human"`, `"representative"`.
* If any such keyword is found ‚Üí route to `"human_agent"`.
* Otherwise, map `intent` to department node names:

  * `"billing"` ‚Üí `"billing_support"`
  * `"technical"` ‚Üí `"tech_support"`
  * `"sales"` ‚Üí `"sales_support"`
  * `"general"` ‚Üí `"general_support"`

This function is used as a **conditional router** in the graph.

---

### Step 6 ‚Äì Department Handlers + Human Agent Escalation

Your code:

* `billing_support(state)`
* `tech_support(state)`
* `sales_support(state)`
* `general_support(state)`
* `human_agent(state)`

**What learners do**

* Paste and run all these functions together.

**Explanation (common pattern)**

Each department function:

* Instantiates `ChatOpenAI` with small temperature (0.3) to keep answers focused.
* Builds a prompt including:

  * Short conversation context (last few messages from `conversation_history`).
  * Current `user_query`.
  * Clear instructions on what kind of issues this department handles.
  * Style constraints: empathetic, conversational, under 3 sentences.
* Calls `llm.invoke(prompt)` and gets a response.
* Appends the user query and assistant reply to `conversation_history`.
* Returns a dict including:

  * `"response"`: text to show the user.
  * `"department"`: `"billing"`, `"technical"`, `"sales"`, or `"general"`.
  * `"escalated": False`
  * `"conversation_history"`: updated list.
  * `"resolution_status": "pending"`
  * `"should_continue": True` (department answers normally ‚Üí conversation can continue).

**Explanation ‚Äì `human_agent(state)`**

* Used for escalations.
* Prompt acknowledges the complaint is serious and explains escalation to a human specialist.
* Returns:

  * `"department": "human_agent"`
  * `"escalated": True`
  * `"needs_human": True`
  * `"resolution_status": "escalated"`
  * `"should_continue": False` (this ends the AI conversation ‚Äî waiting for human).

This models a **realistic support environment** where some cases must go to a human.

---

### Step 7 ‚Äì Conversation Flow Control and Ending Node

Your code:

```python
# Step 7: Build SIMPLE Graph without Recursion
def should_continue(state: SupportState):
    ...

def end_conversation(state: SupportState):
    ...
    return {...}
```

**What learners do**

* Run these function definitions.

**Explanation ‚Äì `should_continue(state)`**

* Checks `user_query` for end phrases:

  * `"thanks"`, `"thank you"`, `"that's all"`, `"no more"`, `"exit"`, `"bye"`, `"done"`.
* If any of those appear ‚Üí `"end_conversation"`.
* Otherwise, looks at `state["should_continue"]`:

  * If `True` ‚Üí `"continue"` (graph finishes this turn).
  * If `False` ‚Üí `"end_conversation"` (go to closing).

**Explanation ‚Äì `end_conversation(state)`**

* Uses `ChatOpenAI` to generate a short, warm closing message.
* Returns:

  * `"response"`: the closing text.
  * `"resolution_status": "resolved"`
  * `"should_continue": False`.

This gives a **clean exit** for the conversation.

---

### Step 8 ‚Äì Build and Compile the Graph

Your code:

```python
# Build the graph WITHOUT recursive follow-up
builder = StateGraph(SupportState)

# Add nodes
builder.add_node("classify_intent", classify_intent)
builder.add_node("billing_support", billing_support)
builder.add_node("tech_support", tech_support)
builder.add_node("sales_support", sales_support)
builder.add_node("general_support", general_support)
builder.add_node("human_agent", human_agent)
builder.add_node("end_conversation", end_conversation)

# Set up the workflow - SIMPLE version
builder.set_entry_point("classify_intent")

# Route from classification to appropriate department
builder.add_conditional_edges(
    "classify_intent",
    route_to_department,
    {
        "billing_support": "billing_support",
        "tech_support": "tech_support",
        "sales_support": "sales_support",
        "general_support": "general_support",
        "human_agent": "human_agent"
    }
)

# After each department, check if we should continue
builder.add_conditional_edges(
    "billing_support", should_continue, {"continue": END, "end_conversation": "end_conversation"}
)
...
builder.add_edge("end_conversation", END)

# Compile the graph with recursion limit
graph = builder.compile()
```

**What learners do**

1. Paste this whole block after the node definitions.
2. Run the cell to build and compile the graph.

**Explanation**

* Nodes added: intent classifier, 4 department handlers, human agent, and closing node.
* Entry point: `"classify_intent"`.
* From `"classify_intent"`, `route_to_department` decides which node to call.
* After each department node, `should_continue` decides:

  * `"continue"` ‚Üí stop graph for this turn (END). The outer chat loop can ask the user for another question.
  * `"end_conversation"` ‚Üí go to `"end_conversation"` node, which generates the closing message, then END.
* `graph = builder.compile()` finalizes the routing logic into an executable graph.

This design intentionally avoids recursive loops **inside** the graph; you loop at the Python level instead (`while True` in `run_interactive_support`).

---

### Step 9 ‚Äì Interactive Support Chat Interface

Your code:

```python
# Step 8: Fixed Interactive Chat Interface
def run_interactive_support():
    ...
```

**What learners do**

1. Run the cell defining `run_interactive_support()`.
2. In a separate cell, call:

   ```python
   run_interactive_support()
   ```
3. Try different queries: billing, tech, sales, general, and escalations.

**Explanation**

* Prints a nice header explaining usage.
* Sets an initial `state` with default values and empty `conversation_history`.
* Enters a `while True` loop:

  * Gets user input via `get_user_input()`.
  * If user types `exit` or a ‚Äúthank you‚Äù-type ending, exits the loop politely.
  * Otherwise:

    * Sets `state["user_query"] = user_input`.
    * Calls `graph.invoke(state)` to process a **single turn**.
    * Prints the department name and the response.
    * Updates `state` with the returned result for the next turn.
* Stops the conversation if:

  * `should_continue` is `False`, or
  * `resolution_status == "resolved"`, or
  * a safety limit (10 turns) is reached.

This is the **main interactive experience** for learners.

---

### Step 10 ‚Äì Automated Test Function

Your code:

```python
# Step 9: Simple Test Function
def test_support_system():
    ...
```

**What learners do**

1. Run the cell defining `test_support_system()`.
2. In a new cell, call:

   ```python
   test_support_system()
   ```

**Explanation**

* Defines a list of sample queries:

  * Technical issue, billing issue, app login problem, sales question, escalation demand.
* For each query:

  * Sets up a fresh `state`.
  * Calls `graph.invoke(state)`.
  * Prints:

    * Detected `department`
    * Generated `response`
    * Whether `escalated` is `True` or `False`.
* Great for **quick regression tests** and demos without user typing.

---

### Step 11 ‚Äì Running the System

Your code prints:

```python
# Step 10: Run the system
print("üöÄ Enhanced Interactive Support System Ready!")
print("\nChoose an option:")
print("1. run_interactive_support() - Chat with the support agent")
print("2. test_support_system() - Run automated tests")
```

**What learners do**

* After running all code cells, they can choose:

  * For manual chat:

    ```python
    run_interactive_support()
    ```

  * For automated tests:

    ```python
    test_support_system()
    ```
